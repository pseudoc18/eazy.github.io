<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Hallucinatory Image Tokens: A Training-free EAZY Approach on Detecting and Mitigating Object Hallucinations in LVLMs">
  <meta name="keywords" content="Hallucination Detection and Mitigation in LVLMs">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Hallucinatory Image Tokens: A Training-free EAZY Approach on Detecting and Mitigating Object Hallucinations in LVLMs</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
<!--  <link rel="icon" href="./static/images/favicon.svg">-->

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Hallucinatory Image Tokens: A Training-free EAZY Approach on Detecting and Mitigating Object Hallucinations in LVLMs</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="#">Liwei Che</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="#">Tony Qingze Liu</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="#">Jing Jia</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="#">Weiyi Qin</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://www.ruixiangtang.net/">Ruixiang Tang</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://chenghan111.github.io/">Vladimir Pavlovic</a><sup>1</sup>,
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Rutgers University</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/pdf/2503.07772"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://arxiv.org/pdf/2503.07772"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <span class="link-block">
                <a href="#"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!--<section class="section">-->
<!--  <div class="container is-max-desktop">-->
<!--    &lt;!&ndash; Abstract. &ndash;&gt;-->
<!--    <div class="columns is-centered has-text-centered">-->
<!--      <div class="column is-four-fifths">-->
<!--        <h2 class="title is-3">Abstract</h2>-->
<!--        <div class="content has-text-justified">-->
<!--          <p>-->
<!--            Recently in robotics, Vision-Language-Action (VLA) models have emerged as a transformative approach, enabling robots to execute complex tasks by integrating visual and linguistic inputs within an end-to-end learning framework. While VLA models offer significant capabilities, they also introduce new attack surfaces, making them vulnerable to adversarial attacks. With these vulnerabilities largely unexplored, this paper systematically quantifies the robustness of VLA-based robotic systems. Recognizing the unique demands of robotic execution,-->
<!--            our attack objectives target the inherent spatial and functional characteristics of robotic systems.-->
<!--            In particular, we introduce an untargeted position-aware attack objective that leverages spatial foundations to destabilize robotic actions, and a targeted attack objective that manipulates the robotic trajectory. Additionally, we design an adversarial patch generation approach that places a small, colorful patch within the camera's view, effectively executing the attack in both digital and physical environments.-->
<!--            Our evaluation reveals a marked degradation in task success rates, with up to a 100\% reduction across a suite of simulated robotic tasks, highlighting critical security gaps in current VLA architectures. By unveiling these vulnerabilities and proposing actionable evaluation metrics, this work advances both the understanding and enhancement of safety for VLA-based robotic systems, underscoring the necessity for developing robust defense strategies prior to physical-world deployments.-->
<!--          </p>-->
<!--        </div>-->
<!--      </div>-->

<!--          &lt;!&ndash; Main Image &ndash;&gt;-->
<!--    <div class="columns is-centered has-text-centered" style="margin-top: 20px;">-->
<!--      <div class="column is-half">-->
<!--        <img src="./static/images/main.png"-->
<!--             class="interpolation-image"-->
<!--             alt="Interpolation end reference image."-->
<!--             style="max-width: 100%; height: auto; border: 1px solid #ccc; border-radius: 8px;">-->
<!--        <p class="is-bold" style="margin-top: 10px;">Main Figure</p>-->
<!--      </div>-->
<!--    </div>-->
<!--    </div>-->
<!--  </div>-->
<!--</section>-->

<section class="section" style="display: flex; flex-direction: column; align-items: center; text-align: center;">
  <div class="container is-max-desktop" style="width: 100%;">
    <!-- Abstract -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Despite their remarkable potential, Large Vision-Language Models (LVLMs) still face challenges with object hallucination, a problem where their generated outputs mistakenly incorporate objects that do not actually exist. Although most works focus on addressing this issue within the language-model backbone, our work shifts the focus to the image input source, investigating how specific image tokens contribute to hallucinations. Our analysis reveals a striking finding: a small subset of image tokens with high attention scores are the primary drivers of object hallucination. By removing these hallucinatory image tokens (only 1.5% of all image tokens), the issue can be effectively mitigated. This finding holds consistently across different models and datasets. Building on this insight, we introduce EAZY, a novel, training-free method that automatically identifies and Eliminates hAllucinations by Zeroing out hallucinatorY image tokens. We utilize EAZY for unsupervised object hallucination detection, achieving 15% improvement compared to previous methods. Additionally, EAZY demonstrates remarkable effectiveness in mitigating hallucinations while preserving model utility and seamlessly adapting to various LVLM architectures.
          </p>
        </div>
      </div>
    </div>

    <!-- Main Image -->
    <div class="columns is-centered has-text-centered" style="margin-top: 40px;">
      <div class="column" style="width: 100%; text-align: center;">
        <img src="./static/images/zero_out.png"
             alt="Main Figure"
             style="width: 400%; height: auto;">
      </div>
    </div>

    <!-- Caption -->
    <div class="columns is-centered has-text-centered" style="margin-top: -20px;">
      <div class="column is-full" style="max-width: 100%; padding: 0 10%;">
        <p style="text-align: justify; font-size: 1rem; line-height: 1.5; margin: 0;">
          EAZY identifies and removes hallucinatory image tokens from the input image, effectively mitigating hallucinations in LVLMs. The method is training-free and can be applied to various LVLM architectures. In this example, EAZY removes three image tokens, which eliminates the hallucinated objects, "apples" and "oranges", revealing the real object "kiwis".
        </p>
        <p style="font-size: 0.9rem; margin-top: 10px; font-style: italic; color: #666;">
          Figure 1: Removing three image tokens results in the elimination of the hallucinated objects, ”apples” and ”oranges”, and reveals the real
object ”kiwis”.
        </p>
      </div>
    </div>
  </div>
</section>




<section class="section">
  <div class="container is-max-desktop">
    <h2 class="title">Experiments</h2>
    <div class="content has-text-justified">
      <p>
        We conduct extensive experiments to evaluate the effectiveness of EAZY in detecting and mitigating object hallucinations across various LVLMs and datasets. Our results demonstrate that EAZY significantly outperforms existing methods in both detection accuracy and mitigation effectiveness, while maintaining model utility.
      </p>
    </div>

    <!-- Experimental Results Grid -->
    <div class="columns is-multiline is-centered" style="margin-top: 30px;">
      
      <!-- chair -->
      <div class="column is-4">
        <div class="has-text-centered">
          <img src="./static/images/chair_exp.png"
               alt="Validation Results"
               style="width: 100%; height: auto; border: 1px solid #ddd; border-radius: 8px;">
          <p class="has-text-weight-semibold" style="margin-top: 15px;">CHAIR Results</p>
          <p class="is-size-7 has-text-grey">
            EAZY's validation results show a significant reduction in hallucination rates across multiple LVLMs, demonstrating its robustness and adaptability.
          </p>
        </div>
      </div>

      <!-- POPE result -->
      <div class="column is-4">
        <div class="has-text-centered">
          <img src="./static/images/pope_exp.png"
               alt="Model Performance Comparison"
               style="width: 100%; height: auto; border: 1px solid #ddd; border-radius: 8px;">
          <p class="has-text-weight-semibold" style="margin-top: 15px;">POPE Results</p>
          <p class="is-size-7 has-text-grey">
            POPE results indicate that EAZY effectiveness.
          </p>
        </div>
      </div>

      <!-- Detection Performance -->
      <div class="column is-4">
        <div class="has-text-centered">
          <img src="./static/images/detection_exp.png"
               alt="Detection Performance"
               style="width: 100%; height: auto; border: 1px solid #ddd; border-radius: 8px;">
          <p class="has-text-weight-semibold" style="margin-top: 15px;">Detection Performance</p>
          <p class="is-size-7 has-text-grey">
            EAZY achieves superior detection accuracy across multiple LVLM architectures, consistently outperforming baseline methods.
          </p>
        </div>
      </div>

    </div>

    <!-- Additional Results Section -->
    <div class="content has-text-justified" style="margin-top: 40px;">
      <h3 class="title is-4">Key Results</h3>
      <ul>
        <li><strong>Detection Accuracy:</strong> EAZY achieves 15% improvement in unsupervised object hallucination detection compared to previous methods.</li>
        <li><strong>Mitigation Effectiveness:</strong> By removing only 1.5% of image tokens, EAZY effectively mitigates hallucinations while preserving model utility.</li>
        <li><strong>Cross-Model Generalization:</strong> The method demonstrates consistent performance across various LVLM architectures including LLaVA, Shikra, and LLaVA-Next.</li>
        <li><strong>Training-Free Approach:</strong> EAZY requires no additional training, making it easily adaptable to existing models.</li>
      </ul>
    </div>
  </div>
</section>

<!-- Case Study Section -->
<section class="section">
  <div class="container is-max-desktop">
    <h2 class="title">Case Studies</h2>
    <div class="content has-text-justified">
      <p>
        We present qualitative case studies demonstrating EAZY's effectiveness in detecting and mitigating object hallucinations across diverse scenarios. These examples showcase how our method identifies problematic image tokens and successfully eliminates hallucinated objects while preserving the accuracy of real object detection.
      </p>
    </div>

    <!-- Case Study Images Grid -->
    <div class="columns is-multiline is-centered" style="margin-top: 30px;">
      
      <!-- Case Study 1 -->
      <div class="column is-6">
        <div class="has-text-centered">
          <img src="./static/images/airport_case.png"
               alt="Airport Case Study"
               style="width: 100%; height: auto; border: 1px solid #ddd; border-radius: 8px;">
          <p class="has-text-weight-semibold" style="margin-top: 15px;">Airport Scene</p>
          <p class="is-size-7 has-text-grey">
            EAZY successfully identifies and removes hallucinated objects in complex airport scenes, demonstrating its effectiveness in real-world environments with multiple objects and intricate backgrounds.
          </p>
        </div>
      </div>

      <!-- Case Study 2 -->
      <div class="column is-6">
        <div class="has-text-centered">
          <img src="./static/images/cat_case.png"
               alt="Cat Case Study"
               style="width: 100%; height: auto; border: 1px solid #ddd; border-radius: 8px;">
          <p class="has-text-weight-semibold" style="margin-top: 15px;">Four Cats Lying on the Table</p>
          <p class="is-size-7 has-text-grey">
            In this cat-focused scenario, by removing just one hallucinatory image token, the EAZY output eliminated the hallucinated "glove" and correct the numbers of cats as well as other objects on the table.
          </p>
        </div>
      </div>

    </div>

    <!-- Case Study Summary -->
    <div class="content has-text-justified" style="margin-top: 40px;">
      <h3 class="title is-4">Case Study Insights</h3>
      <ul>
        <li><strong>Diverse Scenarios:</strong> EAZY performs consistently across various scene types including indoor, outdoor, and complex environments.</li>
        <li><strong>Precision:</strong> The method accurately identifies hallucinatory tokens without affecting genuine object detection.</li>
        <li><strong>Robustness:</strong> Demonstrates effectiveness across different object categories and scene complexities.</li>
        <li><strong>Visual Quality:</strong> Maintains image quality and context while removing problematic tokens.</li>
      </ul>
    </div>
  </div>
</section>

<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@misc{che2025hallucinatoryimagetokenstrainingfree,
      title={Hallucinatory Image Tokens: A Training-free EAZY Approach on Detecting and Mitigating Object Hallucinations in LVLMs}, 
      author={Liwei Che and Tony Qingze Liu and Jing Jia and Weiyi Qin and Ruixiang Tang and Vladimir Pavlovic},
      year={2025},
      eprint={2503.07772},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2503.07772}, 
}
</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="https://arxiv.org/pdf/2503.07772">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="#" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            This website is borrowed from the <a
              href="https://github.com/nerfies/nerfies.github.io">source code</a>, we thank the author for his contribution.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
